{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import os \n",
    "\n",
    "from keras.models import *\n",
    "from keras.optimizers import *\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import backend as K\n",
    "from keras.engine import Input, Model\n",
    "from keras.layers import Conv3D, MaxPooling3D, UpSampling3D, Activation, BatchNormalization, PReLU, Deconvolution3D\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 24 ultrasound images and 24 segmentations\n"
     ]
    }
   ],
   "source": [
    "ultrasound_fullname = 'numpy_data/Stacked Arrays/stacked_image_array.npy'\n",
    "segmentation_fullname = 'numpy_data/Stacked Arrays/stacked_segmentation_array.npy'\n",
    "\n",
    "ultrasound_data = np.load(ultrasound_fullname)\n",
    "segmentation_data = np.load(segmentation_fullname)\n",
    "\n",
    "num_ultrasound = ultrasound_data.shape[0]\n",
    "num_segmentation = segmentation_data.shape[0]\n",
    "\n",
    "print(\"\\nFound {} ultrasound images and {} segmentations\".format(num_ultrasound, num_segmentation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading test ultrasound from: numpy_data/Stacked Arrays/test_image_array.npy\n",
      "Reading test segmentation from : numpy_data/Stacked Arrays/test_segmentation_array.npy\n",
      "\n",
      "Found 6 test ultrasound images and 6 segmentations\n"
     ]
    }
   ],
   "source": [
    "test_ultrasound_fullname = 'numpy_data/Stacked Arrays/test_image_array.npy'\n",
    "test_segmentation_fullname = 'numpy_data/Stacked Arrays/test_segmentation_array.npy'\n",
    "\n",
    "print(\"Reading test ultrasound from: {}\".format(test_ultrasound_fullname))\n",
    "print(\"Reading test segmentation from : {}\".format(test_segmentation_fullname))\n",
    "\n",
    "test_ultrasound_data = np.load(test_ultrasound_fullname)\n",
    "test_segmentation_data = np.load(test_segmentation_fullname)\n",
    "\n",
    "num_test_ultrasound = test_ultrasound_data.shape[0]\n",
    "num_test_segmentation = test_segmentation_data.shape[0]\n",
    "\n",
    "print(\"\\nFound {} test ultrasound images and {} segmentations\".format(num_test_ultrasound, num_test_segmentation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch Generator\n",
    "\n",
    "import keras.utils\n",
    "import scipy.ndimage\n",
    "\n",
    "max_rotation_angle = 10\n",
    "\n",
    "class UltrasoundSegmentationBatchGenerator(keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 x_set,\n",
    "                 y_set,\n",
    "                 batch_size,\n",
    "                 image_dimensions=(16, 256, 256, 1),\n",
    "                 shuffle=True,\n",
    "                 n_channels=1,\n",
    "                 n_classes=2):\n",
    "        self.x = x_set\n",
    "        self.y = y_set\n",
    "        self.batch_size = batch_size\n",
    "        self.image_dimensions = image_dimensions\n",
    "        self.shuffle = shuffle\n",
    "        self.n_channels = n_channels\n",
    "        self.n_classes = n_classes\n",
    "        self.number_of_images = self.x.shape[0]\n",
    "        self.indexes = np.arange(self.number_of_images)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return int(np.floor(self.number_of_images / self.batch_size))\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.indexes = np.arange(self.number_of_images)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.x\n",
    "        y = keras.utils.to_categorical(self.y, self.n_classes)\n",
    "\n",
    "        return x, y\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dilated output\n",
    "\n",
    "def dilateStack(segmentation_data, iterations):\n",
    "    return np.array([scipy.ndimage.binary_dilation(y, iterations=iterations) for y in segmentation_data])\n",
    "\n",
    "width = 16\n",
    "segmentation_dilated = dilateStack(segmentation_data[:, :, :, :, 0], width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this if you don't want dilation\n",
    "\n",
    "segmentation_dilated[:, :, :, :] = segmentation_data[:, :, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import (Input, Conv2D, Conv2DTranspose,\n",
    "                            MaxPooling2D, Concatenate, UpSampling2D,\n",
    "                            Conv3D, Conv3DTranspose, MaxPooling3D,\n",
    "                            UpSampling3D)\n",
    "from keras import optimizers as opt\n",
    "\n",
    "\n",
    "def dice_coefficient(y_true, y_pred):\n",
    "    smoothing_factor = 1\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2.0 * intersection + smoothing_factor) / (K.sum(y_true_f) + K.sum(y_pred_f) + smoothing_factor)\n",
    "\n",
    "def loss_dice_coefficient_error(y_true, y_pred):\n",
    "    return -dice_coefficient(y_true, y_pred)\n",
    "\n",
    "\n",
    "def create_unet_model3D(input_image_size,\n",
    "                        n_labels=1,\n",
    "                        layers=4,\n",
    "                        lowest_resolution=16,\n",
    "                        convolution_kernel_size=(5,5,5),\n",
    "                        deconvolution_kernel_size=(5,5,5),\n",
    "                        pool_size=(2,2,2),\n",
    "                        strides=(2,2,2),\n",
    "                        mode='classification',\n",
    "                        output_activation='tanh',\n",
    "                        init_lr=0.0001):\n",
    "    \"\"\"\n",
    "    Create a 3D Unet model\n",
    "    Example\n",
    "    -------\n",
    "    unet_model = create_unet_model3D( (128,128,128,1), 1, 4)\n",
    "    \"\"\"\n",
    "    layers = np.arange(layers)\n",
    "    number_of_classification_labels = n_labels\n",
    "    \n",
    "    inputs = Input(shape=input_image_size)\n",
    "\n",
    "    ## ENCODING PATH ##\n",
    "\n",
    "    encoding_convolution_layers = []\n",
    "    pool = None\n",
    "    for i in range(len(layers)):\n",
    "        number_of_filters = lowest_resolution * 2**(layers[i])\n",
    "\n",
    "        if i == 0:\n",
    "            conv = Conv3D(filters=number_of_filters, \n",
    "                            kernel_size=convolution_kernel_size,\n",
    "                            activation='relu',\n",
    "                            padding='same')(inputs)\n",
    "        else:\n",
    "            conv = Conv3D(filters=number_of_filters, \n",
    "                            kernel_size=convolution_kernel_size,\n",
    "                            activation='relu',\n",
    "                            padding='same')(pool)\n",
    "\n",
    "        encoding_convolution_layers.append(Conv3D(filters=number_of_filters, \n",
    "                                                        kernel_size=convolution_kernel_size,\n",
    "                                                        activation='relu',\n",
    "                                                        padding='same')(conv))\n",
    "\n",
    "        if i < len(layers)-1:\n",
    "            pool = MaxPooling3D(pool_size=pool_size)(encoding_convolution_layers[i])\n",
    "\n",
    "    ## DECODING PATH ##\n",
    "    outputs = encoding_convolution_layers[len(layers)-1]\n",
    "    for i in range(1,len(layers)):\n",
    "        number_of_filters = lowest_resolution * 2**(len(layers)-layers[i]-1)\n",
    "        tmp_deconv = Conv3DTranspose(filters=number_of_filters, kernel_size=deconvolution_kernel_size,\n",
    "                                     padding='same')(outputs)\n",
    "        tmp_deconv = UpSampling3D(size=pool_size)(tmp_deconv)\n",
    "        outputs = Concatenate(axis=4)([tmp_deconv, encoding_convolution_layers[len(layers)-i-1]])\n",
    "\n",
    "        outputs = Conv3D(filters=number_of_filters, kernel_size=convolution_kernel_size, \n",
    "                        activation='relu', padding='same')(outputs)\n",
    "        outputs = Conv3D(filters=number_of_filters, kernel_size=convolution_kernel_size, \n",
    "                        activation='relu', padding='same')(outputs)\n",
    "\n",
    "    if mode == 'classification':\n",
    "        if number_of_classification_labels == 1:\n",
    "            outputs = Conv3D(filters=number_of_classification_labels+1, kernel_size=(1,1,1), \n",
    "                            activation='sigmoid')(outputs)\n",
    "        else:\n",
    "            outputs = Conv3D(filters=number_of_classification_labels, kernel_size=(1,1,1), \n",
    "                            activation='softmax')(outputs)\n",
    "\n",
    "        unet_model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "        if number_of_classification_labels == 1:\n",
    "            unet_model.compile(loss=loss_dice_coefficient_error, \n",
    "                                optimizer=opt.Adam(lr=init_lr), metrics=[dice_coefficient])\n",
    "        else:\n",
    "            unet_model.compile(loss='categorical_crossentropy', \n",
    "                                optimizer=opt.Adam(lr=init_lr), metrics=['accuracy', 'categorical_crossentropy'])\n",
    "    elif mode =='regression':\n",
    "        outputs = Conv3D(filters=number_of_classification_labels, kernel_size=(1,1,1), \n",
    "                        activation=output_activation)(outputs)\n",
    "        unet_model = Model(inputs=inputs, outputs=outputs)\n",
    "        unet_model.compile(loss='mse', optimizer=opt.Adam(lr=init_lr))\n",
    "    else:\n",
    "        raise ValueError('mode must be either `classification` or `regression`')\n",
    "\n",
    "    return unet_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_unet_model3D(input_image_size=(16, 256, 256, 1), n_labels=1, \n",
    "                            layers=4, mode='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model built with 7426850 parameters\n"
     ]
    }
   ],
   "source": [
    "print(\"Model built with {} parameters\".format(model.count_params()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate decay = 4.95e-05\n"
     ]
    }
   ],
   "source": [
    "max_learning_rate = 0.001\n",
    "min_learning_rate = 0.00001\n",
    "num_epochs = 20\n",
    "\n",
    "learning_rate_decay = (max_learning_rate - min_learning_rate) / num_epochs\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.adam(lr=max_learning_rate, decay=learning_rate_decay),\n",
    "               loss= \"binary_crossentropy\",\n",
    "               metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Learning rate decay = {}\".format(learning_rate_decay))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "batch_size = 23\n",
    "\n",
    "training_generator = UltrasoundSegmentationBatchGenerator(ultrasound_data, segmentation_dilated, batch_size)\n",
    "test_generator = UltrasoundSegmentationBatchGenerator(test_ultrasound_data, test_segmentation_data[:, :, :, :, 0], batch_size)\n",
    "\n",
    "training_time_start = datetime.datetime.now()\n",
    "\n",
    "training_log = model.fit_generator(training_generator,\n",
    "                                   validation_data=test_generator,\n",
    "                                   epochs=num_epochs,\n",
    "                                   verbose=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
